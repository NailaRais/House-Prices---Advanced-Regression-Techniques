import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Load the datasets
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Handle missing values
missing_values_train = train_df.isnull().sum().sort_values(ascending=False)
missing_values_train = missing_values_train[missing_values_train > 0]

missing_values_test = test_df.isnull().sum().sort_values(ascending=False)
missing_values_test = missing_values_test[missing_values_test > 0]

# Define features and target variable
X = train_df.drop(columns=['SalePrice', 'Id'])
y = train_df['SalePrice']

# Preprocessing for numerical data
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

# Preprocessing for categorical data
categorical_features = X.select_dtypes(include=['object']).columns
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)])

# Split the data into training and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)

# Preprocess the training and validation data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_valid_preprocessed = preprocessor.transform(X_valid)
X_test_preprocessed = preprocessor.transform(test_df.drop(columns=['Id']))

# Train Random Forest model
rf_model = RandomForestRegressor(random_state=0)
rf_model.fit(X_train_preprocessed, y_train)

# Evaluate Random Forest model
y_pred_rf = rf_model.predict(X_valid_preprocessed)
rf_mae = mean_absolute_error(y_valid, y_pred_rf)
rf_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_rf))
rf_r2 = r2_score(y_valid, y_pred_rf)

# Train Gradient Boosting model
gb_model = GradientBoostingRegressor(random_state=0)
gb_model.fit(X_train_preprocessed, y_train)

# Evaluate Gradient Boosting model
y_pred_gb = gb_model.predict(X_valid_preprocessed)
gb_mae = mean_absolute_error(y_valid, y_pred_gb)
gb_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_gb))
gb_r2 = r2_score(y_valid, y_pred_gb)

# Perform cross-validation on the best model (Random Forest)
cv_scores = cross_val_score(rf_model, X_train_preprocessed, y_train, cv=5, scoring='neg_mean_absolute_error')
cv_mean = -cv_scores.mean()
cv_std = cv_scores.std()

# Make predictions on the test set using the best model
test_predictions = rf_model.predict(X_test_preprocessed)

# Prepare the submission file
submission_df = pd.DataFrame({'Id': test_df['Id'], 'SalePrice': test_predictions})
submission_df.to_csv('submission.csv', index=False)

# Output the evaluation metrics and cross-validation results
rf_mae, rf_rmse, rf_r2, gb_mae, gb_rmse, gb_r2, cv_mean, cv_std
